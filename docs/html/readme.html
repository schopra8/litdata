


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PQBQ3CV');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Transform data at scale. Optimize for fast model training. &mdash; litdata 0.2.34 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://github.com/Lightning-AI/litdata/readme.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="lit-data" href="index.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>

  <script src="https://unpkg.com/react@18/umd/react.development.js" crossorigin></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js" crossorigin></script>
  <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
  <script src="_static/js/react/react.jsx" type="text/babel"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://lightning-ai.github.io/lit-data/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://lightning.ai">Get Started</a>
          </li>

          <li>
            <a href="https://www.Lightning.ai/blog">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/pytorch/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/fabric/stable/">
                  <span class="dropdown-title">Lightning Fabric</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li>

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning.ai">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          

          <li>
            <a href="https://github.com/Lightning-AI/lit-data">GitHub</a>
          </li>

          <li>
            <a href="https://www.lightning.ai/">Lightning AI</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  0.2.34
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Start here</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Transform data at scale. Optimize for fast model training.</a></li>
<li class="toctree-l1"><a class="reference internal" href="#quick-start">Quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="#speed-up-model-training">Speed up model training</a></li>
<li class="toctree-l1"><a class="reference internal" href="#transform-datasets">Transform datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="#use-a-local-or-s3-folder">use a local or S3 folder</a></li>
<li class="toctree-l1"><a class="reference internal" href="#resize-the-input-image">resize the input image</a></li>
<li class="toctree-l1"><a class="reference internal" href="#key-features">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="#initialize-the-streamingdataset-with-the-custom-cache-directory">Initialize the StreamingDataset with the custom cache directory</a></li>
<li class="toctree-l1"><a class="reference internal" href="#for-the-training-dataset-don-t-forget-to-enable-shuffle-and-drop-last">For the training dataset, don’t forget to enable shuffle and drop_last !!!</a></li>
<li class="toctree-l1"><a class="reference internal" href="#read-data-from-aws-s3">Read data from AWS S3</a></li>
<li class="toctree-l1"><a class="reference internal" href="#read-data-from-gcs">Read data from GCS</a></li>
<li class="toctree-l1"><a class="reference internal" href="#read-data-from-azure">Read data from Azure</a></li>
<li class="toctree-l1"><a class="reference internal" href="#iterate-over-the-data">Iterate over the data</a></li>
<li class="toctree-l1"><a class="reference internal" href="#define-a-function-to-convert-the-text-within-the-jsonl-files-into-tokens">1. Define a function to convert the text within the jsonl files into tokens</a></li>
<li class="toctree-l1"><a class="reference internal" href="#increase-by-one-because-we-need-the-next-word-as-well">Increase by one because we need the next word as well</a></li>
<li class="toctree-l1"><a class="reference internal" href="#iterate-over-the-slimpajama-dataset">Iterate over the SlimPajama dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="#mix-slimpajama-data-and-starcoder-data-with-these-proportions">Mix SlimPajama data and Starcoder data with these proportions:</a></li>
<li class="toctree-l1"><a class="reference internal" href="#iterate-over-the-combined-datasets">Iterate over the combined datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="#define-a-function-to-convert-the-text-within-the-parquet-files-into-tokens">1. Define a function to convert the text within the parquet files into tokens</a></li>
<li class="toctree-l1"><a class="reference internal" href="#generate-the-inputs">2. Generate the inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="#store-the-optimized-data-wherever-you-want-under-teamspace-datasets-or-teamspace-s3-connections">3. Store the optimized data wherever you want under “/teamspace/datasets” or “/teamspace/s3_connections”</a></li>
<li class="toctree-l1"><a class="reference internal" href="#initialize-fernetencryption-with-a-password-for-sample-level-encryption">Initialize FernetEncryption with a password for sample-level encryption</a></li>
<li class="toctree-l1"><a class="reference internal" href="#optimize-data-while-applying-encryption">Optimize data while applying encryption</a></li>
<li class="toctree-l1"><a class="reference internal" href="#save-the-encryption-key-to-a-file-for-later-use">Save the encryption key to a file for later use</a></li>
<li class="toctree-l1"><a class="reference internal" href="#load-the-encryption-key">Load the encryption key</a></li>
<li class="toctree-l1"><a class="reference internal" href="#create-a-streaming-dataset-for-reading-the-encrypted-samples">Create a streaming dataset for reading the encrypted samples</a></li>
<li class="toctree-l1"><a class="reference internal" href="#note-inputs-could-also-refer-to-files-on-s3-directly">Note: Inputs could also refer to files on s3 directly.</a></li>
<li class="toctree-l1"><a class="reference internal" href="#files-written-to-output-dir-are-persisted">Files written to output_dir are persisted.</a></li>
<li class="toctree-l1"><a class="reference internal" href="#benchmarks">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="#parallelize-transforms-and-data-optimization-on-cloud-machines">Parallelize transforms and data optimization on cloud machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="#start-from-a-template">Start from a template</a></li>
<li class="toctree-l1"><a class="reference internal" href="#community">Community</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Transform data at scale. Optimize for fast model training.</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/readme.md.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <p>.. container::</p>
<p></p>
<p><strong>Transform datasets at scale.
Optimize data for fast AI model training.</strong></p>
<p>.. raw:: html</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>  &lt;pre&gt;
  Transform                              Optimize
    
  ✅ Parallelize data processing       ✅ Stream large cloud datasets          
  ✅ Create vector embeddings          ✅ Accelerate training by 20x           
  ✅ Run distributed inference         ✅ Pause and resume data streaming      
  ✅ Scrape websites at scale          ✅ Use remote data without local loading
  &lt;/pre&gt;
</pre></div>
</div>
<hr class="docutils" />
<p>|PyPI| |Downloads| |License|</p>
<p>.. raw:: html</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>  &lt;p align=&quot;center&quot;&gt;
</pre></div>
</div>
<p>Lightning AI • Quick start • Optimize data • Transform data •
Features • Benchmarks • Templates • Community</p>
<p>.. raw:: html</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>  &lt;/p&gt;
</pre></div>
</div>
<p></p>
<p></p>
<section id="transform-data-at-scale-optimize-for-fast-model-training">
<h1>Transform data at scale. Optimize for fast model training.<a class="headerlink" href="#transform-data-at-scale-optimize-for-fast-model-training" title="Permalink to this heading">¶</a></h1>
<p>LitData scales <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">processing</span> <span class="pre">tasks</span> <span class="pre">&lt;#transform-datasets&gt;</span></code>__ (data
scraping, image resizing, distributed inference, embedding creation) on
local or cloud machines. It also enables <code class="docutils literal notranslate"><span class="pre">optimizing</span> <span class="pre">datasets</span> <span class="pre">&lt;#speed-up-model-training&gt;</span></code>__ to accelerate AI model training
and work with large remote datasets without local loading.</p>
<p></p>
</section>
<section id="quick-start">
<h1>Quick start<a class="headerlink" href="#quick-start" title="Permalink to this heading">¶</a></h1>
<p>First, install LitData:</p>
<p>.. code:: bash</p>
<p>pip install litdata</p>
<p>Choose your workflow:</p>
<p>| 🚀 <code class="docutils literal notranslate"><span class="pre">Speed</span> <span class="pre">up</span> <span class="pre">model</span> <span class="pre">training</span> <span class="pre">&lt;#speed-up-model-training&gt;</span></code>__
| 🚀 <code class="docutils literal notranslate"><span class="pre">Transform</span> <span class="pre">datasets</span> <span class="pre">&lt;#transform-datasets&gt;</span></code>__</p>
<p></p>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>Advanced install</p>
<p>.. raw:: html</p>
   </summary>
<p>Install all the extras</p>
<p>.. code:: bash</p>
<p>pip install ‘litdata[extras]’</p>
<p>.. raw:: html</p>
   </details>
<p></p>
</section>
<hr class="docutils" />
<section id="speed-up-model-training">
<h1>Speed up model training<a class="headerlink" href="#speed-up-model-training" title="Permalink to this heading">¶</a></h1>
<p>Accelerate model training (20x faster) by optimizing datasets for
streaming directly from cloud storage. Work with remote data without
local downloads with features like loading data subsets, accessing
individual samples, and resumable streaming.</p>
<p><strong>Step 1: Optimize the data</strong> This step will format the dataset for fast
loading. The data will be written in a chunked binary format.</p>
<p>.. code:: python</p>
<p>import numpy as np
from PIL import Image
import litdata as ld</p>
<p>def random_images(index):
fake_images = Image.fromarray(np.random.randint(0, 256, (32, 32, 3), dtype=np.uint8))
fake_labels = np.random.randint(10)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   # You can use any key:value pairs. Note that their types must not change between samples, and Python lists must
   # always contain the same number of elements with the same types.
   data = {&quot;index&quot;: index, &quot;image&quot;: fake_images, &quot;class&quot;: fake_labels}

   return data
</pre></div>
</div>
<p>if <strong>name</strong> == “<strong>main</strong>”:
# The optimize function writes data in an optimized format.
ld.optimize(
fn=random_images,                   # the function applied to each input
inputs=list(range(1000)),           # the inputs to the function (here it’s a list of numbers)
output_dir=”fast_data”,             # optimized data is stored here
num_workers=4,                      # The number of workers on the same machine
chunk_bytes=”64MB”                  # size of each chunk
)</p>
<p><strong>Step 2: Put the data on the cloud</strong></p>
<p>Upload the data to a <code class="docutils literal notranslate"><span class="pre">Lightning</span> <span class="pre">Studio</span> <span class="pre">&lt;https://lightning.ai&gt;</span></code>__ (backed
by S3) or your own S3 bucket:</p>
<p>.. code:: bash</p>
<p>aws s3 cp –recursive fast_data s3://my-bucket/fast_data</p>
<p><strong>Step 3: Stream the data during training</strong></p>
<p>Load the data by replacing the PyTorch DataSet and DataLoader with the
StreamingDataset and StreamingDataloader</p>
<p>.. code:: python</p>
<p>import litdata as ld</p>
<p>train_dataset = ld.StreamingDataset(‘s3://my-bucket/fast_data’, shuffle=True, drop_last=True)
train_dataloader = ld.StreamingDataLoader(train_dataset)</p>
<p>for sample in train_dataloader:
img, cls = sample[‘image’], sample[‘class’]</p>
<p><strong>Key benefits:</strong></p>
<p>| ✅ Accelerate training: Optimized datasets load 20x faster.
| ✅ Stream cloud datasets: Work with cloud data without downloading it.
| ✅ Pytorch-first: Works with PyTorch libraries like PyTorch Lightning,
Lightning Fabric, Hugging Face.
| ✅ Easy collaboration: Share and access datasets in the cloud,
streamlining team projects.
| ✅ Scale across GPUs: Streamed data automatically scales to all GPUs.
| ✅ Flexible storage: Use S3, GCS, Azure, or your own cloud account for
data storage.
| ✅ Compression: Reduce your data footprint by using advanced
compression algorithms.
| ✅ Run local or cloud: Run on your own machines or auto-scale to 1000s
of cloud GPUs with Lightning Studios.
| ✅ Enterprise security: Self host or process data on your cloud
account with Lightning Studios.</p>
<p></p>
</section>
<hr class="docutils" />
<section id="transform-datasets">
<h1>Transform datasets<a class="headerlink" href="#transform-datasets" title="Permalink to this heading">¶</a></h1>
<p>Accelerate data processing tasks (data scraping, image resizing,
embedding creation, distributed inference) by parallelizing (map) the
work across many machines at once.</p>
<p>Here’s an example that resizes and crops a large image dataset:</p>
<p>.. code:: python</p>
<p>from PIL import Image
import litdata as ld</p>
</section>
<section id="use-a-local-or-s3-folder">
<h1>use a local or S3 folder<a class="headerlink" href="#use-a-local-or-s3-folder" title="Permalink to this heading">¶</a></h1>
<p>input_dir = “my_large_images”     # or “s3://my-bucket/my_large_images”
output_dir = “my_resized_images”  # or “s3://my-bucket/my_resized_images”</p>
<p>inputs = [os.path.join(input_dir, f) for f in os.listdir(input_dir)]</p>
</section>
<section id="resize-the-input-image">
<h1>resize the input image<a class="headerlink" href="#resize-the-input-image" title="Permalink to this heading">¶</a></h1>
<p>def resize_image(image_path, output_dir):
output_image_path = os.path.join(output_dir, os.path.basename(image_path))
Image.open(image_path).resize((224, 224)).save(output_image_path)</p>
<p>ld.map(
fn=resize_image,
inputs=inputs,
output_dir=”output_dir”,
)</p>
<p><strong>Key benefits:</strong></p>
<p>| ✅ Parallelize processing: Reduce processing time by transforming data
across multiple machines simultaneously.
| ✅ Scale to large data: Increase the size of datasets you can
efficiently handle.
| ✅ Flexible usecases: Resize images, create embeddings, scrape the
internet, etc…
| ✅ Run local or cloud: Run on your own machines or auto-scale to 1000s
of cloud GPUs with Lightning Studios.
| ✅ Enterprise security: Self host or process data on your cloud
account with Lightning Studios.</p>
<p></p>
</section>
<hr class="docutils" />
<section id="key-features">
<h1>Key Features<a class="headerlink" href="#key-features" title="Permalink to this heading">¶</a></h1>
<section id="features-for-optimizing-and-streaming-datasets-for-model-training">
<h2>Features for optimizing and streaming datasets for model training<a class="headerlink" href="#features-for-optimizing-and-streaming-datasets-for-model-training" title="Permalink to this heading">¶</a></h2>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Stream large cloud datasets</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Use data stored on the cloud without needing to download it all to your
computer, saving time and space.</p>
<p>Imagine you’re working on a project with a huge amount of data stored
online. Instead of waiting hours to download it all, you can start
working with the data almost immediately by streaming it.</p>
<p>Once you’ve optimized the dataset with LitData, stream it as follows:</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset, StreamingDataLoader</p>
<p>dataset = StreamingDataset(‘s3://my-bucket/my-data’, shuffle=True)
dataloader = StreamingDataLoader(dataset, batch_size=64)</p>
<p>for batch in dataloader:
process(batch)  # Replace with your data processing logic</p>
<p>Additionally, you can inject client connection settings for
<code class="docutils literal notranslate"><span class="pre">S3</span> <span class="pre">&lt;https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html#boto3.session.Session.client&gt;</span></code>__
or GCP when initializing your dataset. This is useful for specifying
custom endpoints and credentials per dataset.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset</p>
<p>storage_options = {
“endpoint_url”: “your_endpoint_url”,
“aws_access_key_id”: “your_access_key_id”,
“aws_secret_access_key”: “your_secret_access_key”,
}</p>
<p>dataset = StreamingDataset(‘s3://my-bucket/my-data’, storage_options=storage_options)</p>
<p>Also, you can specify a custom cache directory when initializing your
dataset. This is useful when you want to store the cache in a specific
location.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset</p>
</section>
</section>
<section id="initialize-the-streamingdataset-with-the-custom-cache-directory">
<h1>Initialize the StreamingDataset with the custom cache directory<a class="headerlink" href="#initialize-the-streamingdataset-with-the-custom-cache-directory" title="Permalink to this heading">¶</a></h1>
<p>dataset = StreamingDataset(‘s3://my-bucket/my-data’, cache_dir=”/path/to/cache”)</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Streams on multi-GPU, multi-node</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Data optimized and loaded with Lightning automatically streams
efficiently in distributed training across GPUs or multi-node.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">StreamingDataset</span></code> and <code class="docutils literal notranslate"><span class="pre">StreamingDataLoader</span></code> automatically make
sure each rank receives the same quantity of varied batches of data, so
it works out of the box with your favorite frameworks (<code class="docutils literal notranslate"><span class="pre">PyTorch</span> <span class="pre">Lightning</span> <span class="pre">&lt;https://lightning.ai/docs/pytorch/stable/&gt;</span></code><strong>, <code class="docutils literal notranslate"><span class="pre">Lightning</span> <span class="pre">Fabric</span> <span class="pre">&lt;https://lightning.ai/docs/fabric/stable/&gt;</span></code></strong>, or
<code class="docutils literal notranslate"><span class="pre">PyTorch</span> <span class="pre">&lt;https://pytorch.org/docs/stable/index.html&gt;</span></code>__) to do
distributed training.</p>
<p>Here you can see an illustration showing how the Streaming Dataset works
with multi node / multi gpu under the hood.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset, StreamingDataLoader</p>
</section>
<section id="for-the-training-dataset-don-t-forget-to-enable-shuffle-and-drop-last">
<h1>For the training dataset, don’t forget to enable shuffle and drop_last !!!<a class="headerlink" href="#for-the-training-dataset-don-t-forget-to-enable-shuffle-and-drop-last" title="Permalink to this heading">¶</a></h1>
<p>train_dataset = StreamingDataset(‘s3://my-bucket/my-train-data’, shuffle=True, drop_last=True)
train_dataloader = StreamingDataLoader(train_dataset, batch_size=64)</p>
<p>for batch in train_dataloader:
process(batch)  # Replace with your data processing logic</p>
<p>val_dataset = StreamingDataset(‘s3://my-bucket/my-val-data’, shuffle=False, drop_last=False)
val_dataloader = StreamingDataLoader(val_dataset, batch_size=64)</p>
<p>for batch in val_dataloader:
process(batch)  # Replace with your data processing logic</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Stream from multiple cloud providers</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>The StreamingDataset supports reading optimized datasets from common
cloud providers.</p>
<p>.. code:: python</p>
<p>import os
import litdata as ld</p>
</section>
<section id="read-data-from-aws-s3">
<h1>Read data from AWS S3<a class="headerlink" href="#read-data-from-aws-s3" title="Permalink to this heading">¶</a></h1>
<p>aws_storage_options={
“AWS_ACCESS_KEY_ID”: os.environ[‘AWS_ACCESS_KEY_ID’],
“AWS_SECRET_ACCESS_KEY”: os.environ[‘AWS_SECRET_ACCESS_KEY’],
}
dataset = ld.StreamingDataset(“s3://my-bucket/my-data”, storage_options=aws_storage_options)</p>
</section>
<section id="read-data-from-gcs">
<h1>Read data from GCS<a class="headerlink" href="#read-data-from-gcs" title="Permalink to this heading">¶</a></h1>
<p>gcp_storage_options={
“project”: os.environ[‘PROJECT_ID’],
}
dataset = ld.StreamingDataset(“gs://my-bucket/my-data”, storage_options=gcp_storage_options)</p>
</section>
<section id="read-data-from-azure">
<h1>Read data from Azure<a class="headerlink" href="#read-data-from-azure" title="Permalink to this heading">¶</a></h1>
<p>azure_storage_options={
“account_url”: f”https://{os.environ[‘AZURE_ACCOUNT_NAME’]}.blob.core.windows.net”,
“credential”: os.environ[‘AZURE_ACCOUNT_ACCESS_KEY’]
}
dataset = ld.StreamingDataset(“azure://my-bucket/my-data”, storage_options=azure_storage_options)</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Pause, resume data streaming</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Stream data during long training, if interrupted, pick up right where
you left off without any issues.</p>
<p>LitData provides a stateful <code class="docutils literal notranslate"><span class="pre">Streaming</span> <span class="pre">DataLoader</span></code> e.g. you can
<code class="docutils literal notranslate"><span class="pre">pause</span></code> and <code class="docutils literal notranslate"><span class="pre">resume</span></code> your training whenever you want.</p>
<p>Info: The <code class="docutils literal notranslate"><span class="pre">Streaming</span> <span class="pre">DataLoader</span></code> was used by
<code class="docutils literal notranslate"><span class="pre">Lit-GPT</span> <span class="pre">&lt;https://github.com/Lightning-AI/lit-gpt/blob/main/pretrain/tinyllama.py&gt;</span></code>__
to pretrain LLMs. Restarting from an older checkpoint was critical to
get to pretrain the full model due to several failures (network, CUDA
Errors, etc..).</p>
<p>.. code:: python</p>
<p>import os
import torch
from litdata import StreamingDataset, StreamingDataLoader</p>
<p>dataset = StreamingDataset(“s3://my-bucket/my-data”, shuffle=True)
dataloader = StreamingDataLoader(dataset, num_workers=os.cpu_count(), batch_size=64)</p>
<p># Restore the dataLoader state if it exists
if os.path.isfile(“dataloader_state.pt”):
state_dict = torch.load(“dataloader_state.pt”)
dataloader.load_state_dict(state_dict)</p>
</section>
<section id="iterate-over-the-data">
<h1>Iterate over the data<a class="headerlink" href="#iterate-over-the-data" title="Permalink to this heading">¶</a></h1>
<p>for batch_idx, batch in enumerate(dataloader):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   # Store the state every 1000 batches
   if batch_idx % 1000 == 0:
       torch.save(dataloader.state_dict(), &quot;dataloader_state.pt&quot;)
</pre></div>
</div>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ LLM Pre-training</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>LitData is highly optimized for LLM pre-training. First, we need to
tokenize the entire dataset and then we can consume it.</p>
<p>.. code:: python</p>
<p>import json
from pathlib import Path
import zstandard as zstd
from litdata import optimize, TokensLoader
from tokenizer import Tokenizer
from functools import partial</p>
</section>
<section id="define-a-function-to-convert-the-text-within-the-jsonl-files-into-tokens">
<h1>1. Define a function to convert the text within the jsonl files into tokens<a class="headerlink" href="#define-a-function-to-convert-the-text-within-the-jsonl-files-into-tokens" title="Permalink to this heading">¶</a></h1>
<p>def tokenize_fn(filepath, tokenizer=None):
with zstd.open(open(filepath, “rb”), “rt”, encoding=”utf-8”) as f:
for row in f:
text = json.loads(row)[“text”]
if json.loads(row)[“meta”][“redpajama_set_name”] == “RedPajamaGithub”:
continue  # exclude the GitHub data since it overlaps with starcoder
text_ids = tokenizer.encode(text, bos=False, eos=True)
yield text_ids</p>
<p>if <strong>name</strong> == “<strong>main</strong>”:
# 2. Generate the inputs (we are going to optimize all the compressed json files from SlimPajama dataset )
input_dir = “./slimpajama-raw”
inputs = [str(file) for file in Path(f”{input_dir}/SlimPajama-627B/train”).rglob(“*.zst”)]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   # 3. Store the optimized data wherever you want under &quot;/teamspace/datasets&quot; or &quot;/teamspace/s3_connections&quot;
   outputs = optimize(
       fn=partial(tokenize_fn, tokenizer=Tokenizer(f&quot;{input_dir}/checkpoints/Llama-2-7b-hf&quot;)), # Note: You can use HF tokenizer or any others
       inputs=inputs,
       output_dir=&quot;./slimpajama-optimized&quot;,
       chunk_size=(2049 * 8012),
       # This is important to inform LitData that we are encoding contiguous 1D array (tokens). 
       # LitData skips storing metadata for each sample e.g all the tokens are concatenated to form one large tensor.
       item_loader=TokensLoader(),
   )
</pre></div>
</div>
<p>.. code:: python</p>
<p>import os
from litdata import StreamingDataset, StreamingDataLoader, TokensLoader
from tqdm import tqdm</p>
</section>
<section id="increase-by-one-because-we-need-the-next-word-as-well">
<h1>Increase by one because we need the next word as well<a class="headerlink" href="#increase-by-one-because-we-need-the-next-word-as-well" title="Permalink to this heading">¶</a></h1>
<p>dataset = StreamingDataset(
input_dir=f”./slimpajama-optimized/train”,
item_loader=TokensLoader(block_size=2048 + 1),
shuffle=True,
drop_last=True,
)</p>
<p>train_dataloader = StreamingDataLoader(dataset, batch_size=8, pin_memory=True, num_workers=os.cpu_count())</p>
</section>
<section id="iterate-over-the-slimpajama-dataset">
<h1>Iterate over the SlimPajama dataset<a class="headerlink" href="#iterate-over-the-slimpajama-dataset" title="Permalink to this heading">¶</a></h1>
<p>for batch in tqdm(train_dataloader):
pass</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Filter illegal data</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Sometimes, you have bad data that you don’t want to include in the
optimized dataset. With LitData, yield only the good data sample to
include.</p>
<p>.. code:: python</p>
<p>from litdata import optimize, StreamingDataset</p>
<p>def should_keep(index) -&gt; bool:
# Replace with your own logic
return index % 2 == 0</p>
<p>def fn(data):
if should_keep(data):
yield data</p>
<p>if <strong>name</strong> == “<strong>main</strong>”:
optimize(
fn=fn,
inputs=list(range(1000)),
output_dir=”only_even_index_optimized”,
chunk_bytes=”64MB”,
num_workers=1
)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   dataset = StreamingDataset(&quot;only_even_index_optimized&quot;)
   data = list(dataset)
   print(data)
   # [0, 2, 4, 6, 8, 10, ..., 992, 994, 996, 998]
</pre></div>
</div>
<p>You can even use try/expect.</p>
<p>.. code:: python</p>
<p>from litdata import optimize, StreamingDataset</p>
<p>def fn(data):
try:
yield 1 / data
except:
pass</p>
<p>if <strong>name</strong> == “<strong>main</strong>”:
optimize(
fn=fn,
inputs=[0, 0, 0, 1, 2, 4, 0],
output_dir=”only_defined_ratio_optimized”,
chunk_bytes=”64MB”,
num_workers=1
)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   dataset = StreamingDataset(&quot;only_defined_ratio_optimized&quot;)
   data = list(dataset)
   # The 0 are filtered out as they raise a division by zero 
   print(data)
   # [1.0, 0.5, 0.25] 
</pre></div>
</div>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Combine datasets</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Mix and match different sets of data to experiment and create better
models.</p>
<p>Combine datasets with <code class="docutils literal notranslate"><span class="pre">CombinedStreamingDataset</span></code>. As an example, this
mixture of
<code class="docutils literal notranslate"><span class="pre">Slimpajama</span> <span class="pre">&lt;https://huggingface.co/datasets/cerebras/SlimPajama-627B&gt;</span></code>__
&amp; <code class="docutils literal notranslate"><span class="pre">StarCoder</span> <span class="pre">&lt;https://huggingface.co/datasets/bigcode/starcoderdata&gt;</span></code>__
was used in the <code class="docutils literal notranslate"><span class="pre">TinyLLAMA</span> <span class="pre">&lt;https://github.com/jzhang38/TinyLlama&gt;</span></code>__
project to pretrain a 1.1B Llama model on 3 trillion tokens.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset, CombinedStreamingDataset, StreamingDataLoader, TokensLoader
from tqdm import tqdm
import os</p>
<p>train_datasets = [
StreamingDataset(
input_dir=”s3://tinyllama-template/slimpajama/train/”,
item_loader=TokensLoader(block_size=2048 + 1), # Optimized loader for tokens used by LLMs
shuffle=True,
drop_last=True,
),
StreamingDataset(
input_dir=”s3://tinyllama-template/starcoder/”,
item_loader=TokensLoader(block_size=2048 + 1), # Optimized loader for tokens used by LLMs
shuffle=True,
drop_last=True,
),
]</p>
</section>
<section id="mix-slimpajama-data-and-starcoder-data-with-these-proportions">
<h1>Mix SlimPajama data and Starcoder data with these proportions:<a class="headerlink" href="#mix-slimpajama-data-and-starcoder-data-with-these-proportions" title="Permalink to this heading">¶</a></h1>
<p>weights = (0.693584, 0.306416)
combined_dataset = CombinedStreamingDataset(datasets=train_datasets, seed=42, weights=weights, iterate_over_all=False)</p>
<p>train_dataloader = StreamingDataLoader(combined_dataset, batch_size=8, pin_memory=True, num_workers=os.cpu_count())</p>
</section>
<section id="iterate-over-the-combined-datasets">
<h1>Iterate over the combined datasets<a class="headerlink" href="#iterate-over-the-combined-datasets" title="Permalink to this heading">¶</a></h1>
<p>for batch in tqdm(train_dataloader):
pass</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Merge datasets</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Merge multiple optimized datasets into one.</p>
<p>.. code:: python</p>
<p>import numpy as np
from PIL import Image</p>
<p>from litdata import StreamingDataset, merge_datasets, optimize</p>
<p>def random_images(index):
return {
“index”: index,
“image”: Image.fromarray(np.random.randint(0, 256, (32, 32, 3), dtype=np.uint8)),
“class”: np.random.randint(10),
}</p>
<p>if <strong>name</strong> == “<strong>main</strong>”:
out_dirs = [“fast_data_1”, “fast_data_2”, “fast_data_3”, “fast_data_4”]  # or [“s3://my-bucket/fast_data_1”, etc.]”
for out_dir in out_dirs:
optimize(fn=random_images, inputs=list(range(250)), output_dir=out_dir, num_workers=4, chunk_bytes=”64MB”)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   merged_out_dir = &quot;merged_fast_data&quot; # or &quot;s3://my-bucket/merged_fast_data&quot;
   merge_datasets(input_dirs=out_dirs, output_dir=merged_out_dir)

   dataset = StreamingDataset(merged_out_dir)
   print(len(dataset))
   # out: 1000
</pre></div>
</div>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Split datasets for train, val, test</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Split a dataset into train, val, test splits with <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code>.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset, train_test_split</p>
<p>dataset = StreamingDataset(“s3://my-bucket/my-data”) # data are stored in the cloud</p>
<p>print(len(dataset)) # display the length of your data
# out: 100,000</p>
<p>train_dataset, val_dataset, test_dataset = train_test_split(dataset, splits=[0.3, 0.2, 0.5])</p>
<p>print(train_dataset)
# out: 30,000</p>
<p>print(val_dataset)
# out: 20,000</p>
<p>print(test_dataset)
# out: 50,000</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Load a subset of the remote dataset</p>
<p>.. raw:: html</p>
   </summary>
<p>Work on a smaller, manageable portion of your data to save time and
resources.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset, train_test_split</p>
<p>dataset = StreamingDataset(“s3://my-bucket/my-data”, subsample=0.01) # data are stored in the cloud</p>
<p>print(len(dataset)) # display the length of your data
# out: 1000</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Easily modify optimized cloud datasets</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Add new data to an existing dataset or start fresh if needed, providing
flexibility in data management.</p>
<p>LitData optimized datasets are assumed to be immutable. However, you can
make the decision to modify them by changing the mode to either
<code class="docutils literal notranslate"><span class="pre">append</span></code> or <code class="docutils literal notranslate"><span class="pre">overwrite</span></code>.</p>
<p>.. code:: python</p>
<p>from litdata import optimize, StreamingDataset</p>
<p>def compress(index):
return index, index**2</p>
<p>if <strong>name</strong> == “<strong>main</strong>”:
# Add some data
optimize(
fn=compress,
inputs=list(range(100)),
output_dir=”./my_optimized_dataset”,
chunk_bytes=”64MB”,
)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   # Later on, you add more data
   optimize(
       fn=compress,
       inputs=list(range(100, 200)),
       output_dir=&quot;./my_optimized_dataset&quot;,
       chunk_bytes=&quot;64MB&quot;,
       mode=&quot;append&quot;,
   )

   ds = StreamingDataset(&quot;./my_optimized_dataset&quot;)
   assert len(ds) == 200
   assert ds[:] == [(i, i**2) for i in range(200)]
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> mode will delete the existing data and start from
fresh.</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Use compression</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Reduce your data footprint by using advanced compression algorithms.</p>
<p>.. code:: python</p>
<p>import litdata as ld</p>
<p>def compress(index):
return index, index**2</p>
<p>if <strong>name</strong> == “<strong>main</strong>”:
# Add some data
ld.optimize(
fn=compress,
inputs=list(range(100)),
output_dir=”./my_optimized_dataset”,
chunk_bytes=”64MB”,
num_workers=1,
compression=”zstd”
)</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">zstd</span> <span class="pre">&lt;https://github.com/facebook/zstd&gt;</span></code>__, you can achieve high
compression ratio like 4.34x for this simple example.</p>
<p>======= ====
Without With
======= ====
2.8kb   646b
======= ====</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Access samples without full data download</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Look at specific parts of a large dataset without downloading the whole
thing or loading it on a local machine.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset</p>
<p>dataset = StreamingDataset(“s3://my-bucket/my-data”) # data are stored in the cloud</p>
<p>print(len(dataset)) # display the length of your data</p>
<p>print(dataset[42]) # show the 42th element of the dataset</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Use any data transforms</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Customize how your data is processed to better fit your needs.</p>
<p>Subclass the <code class="docutils literal notranslate"><span class="pre">StreamingDataset</span></code> and override its <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>
method to add any extra data transformations.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset, StreamingDataLoader
import torchvision.transforms.v2.functional as F</p>
<p>class ImagenetStreamingDataset(StreamingDataset):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   def __getitem__(self, index):
       image = super().__getitem__(index)
       return F.resize(image, (224, 224))
</pre></div>
</div>
<p>dataset = ImagenetStreamingDataset(…)
dataloader = StreamingDataLoader(dataset, batch_size=4)</p>
<p>for batch in dataloader:
print(batch.shape)
# Out: (4, 3, 224, 224)</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Profile data loading speed</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Measure and optimize how fast your data is being loaded, improving
efficiency.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">StreamingDataLoader</span></code> supports profiling of your data loading
process. Simply use the <code class="docutils literal notranslate"><span class="pre">profile_batches</span></code> argument to specify the
number of batches you want to profile:</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset, StreamingDataLoader</p>
<p>StreamingDataLoader(…, profile_batches=5)</p>
<p>This generates a Chrome trace called <code class="docutils literal notranslate"><span class="pre">result.json</span></code>. Then, visualize
this trace by opening Chrome browser at the <code class="docutils literal notranslate"><span class="pre">chrome://tracing</span></code> URL and
load the trace inside.</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Reduce memory use for large files</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Handle large data files efficiently without using too much of your
computer’s memory.</p>
<p>When processing large files like compressed <code class="docutils literal notranslate"><span class="pre">parquet</span> <span class="pre">files</span> <span class="pre">&lt;https://en.wikipedia.org/wiki/Apache_Parquet&gt;</span></code>__, use the Python
yield keyword to process and store one item at the time, reducing the
memory footprint of the entire program.</p>
<p>.. code:: python</p>
<p>from pathlib import Path
import pyarrow.parquet as pq
from litdata import optimize
from tokenizer import Tokenizer
from functools import partial</p>
</section>
<section id="define-a-function-to-convert-the-text-within-the-parquet-files-into-tokens">
<h1>1. Define a function to convert the text within the parquet files into tokens<a class="headerlink" href="#define-a-function-to-convert-the-text-within-the-parquet-files-into-tokens" title="Permalink to this heading">¶</a></h1>
<p>def tokenize_fn(filepath, tokenizer=None):
parquet_file = pq.ParquetFile(filepath)
# Process per batch to reduce RAM usage
for batch in parquet_file.iter_batches(batch_size=8192, columns=[“content”]):
for text in batch.to_pandas()[“content”]:
yield tokenizer.encode(text, bos=False, eos=True)</p>
</section>
<section id="generate-the-inputs">
<h1>2. Generate the inputs<a class="headerlink" href="#generate-the-inputs" title="Permalink to this heading">¶</a></h1>
<p>input_dir = “/teamspace/s3_connections/tinyllama-template”
inputs = [str(file) for file in Path(f”{input_dir}/starcoderdata”).rglob(“*.parquet”)]</p>
</section>
<section id="store-the-optimized-data-wherever-you-want-under-teamspace-datasets-or-teamspace-s3-connections">
<h1>3. Store the optimized data wherever you want under “/teamspace/datasets” or “/teamspace/s3_connections”<a class="headerlink" href="#store-the-optimized-data-wherever-you-want-under-teamspace-datasets-or-teamspace-s3-connections" title="Permalink to this heading">¶</a></h1>
<p>outputs = optimize(
fn=partial(tokenize_fn, tokenizer=Tokenizer(f”{input_dir}/checkpoints/Llama-2-7b-hf”)), # Note: Use HF tokenizer or any others
inputs=inputs,
output_dir=”/teamspace/datasets/starcoderdata”,
chunk_size=(2049 * 8012), # Number of tokens to store by chunks. This is roughly 64MB of tokens per chunk.
)</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Limit local cache space</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Limit the amount of disk space used by temporary files, preventing
storage issues.</p>
<p>Adapt the local caching limit of the <code class="docutils literal notranslate"><span class="pre">StreamingDataset</span></code>. This is
useful to make sure the downloaded data chunks are deleted when used and
the disk usage stays low.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset</p>
<p>dataset = StreamingDataset(…, max_cache_size=”10GB”)</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Change cache directory path</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Specify the directory where cached files should be stored, ensuring
efficient data retrieval and management. This is particularly useful for
organizing your data storage and improving access times.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset
from litdata.streaming.cache import Dir</p>
<p>cache_dir = “/path/to/your/cache”
data_dir = “s3://my-bucket/my_optimized_dataset”</p>
<p>dataset = StreamingDataset(input_dir=Dir(path=cache_dir, url=data_dir))</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Optimize loading on networked drives</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Optimize data handling for computers on a local network to improve
performance for on-site setups.</p>
<p>On-prem compute nodes can mount and use a network drive. A network drive
is a shared storage device on a local area network. In order to reduce
their network overload, the <code class="docutils literal notranslate"><span class="pre">StreamingDataset</span></code> supports <code class="docutils literal notranslate"><span class="pre">caching</span></code>
the data chunks.</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset</p>
<p>dataset = StreamingDataset(input_dir=”local:/data/shared-drive/some-data”)</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Optimize dataset in distributed environment</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Lightning can distribute large workloads across hundreds of machines in
parallel. This can reduce the time to complete a data processing task
from weeks to minutes by scaling to enough machines.</p>
<p>To apply the optimize operator across multiple machines, simply provide
the num_nodes and machine arguments to it as follows:</p>
<p>.. code:: python</p>
<p>import os
from litdata import optimize, Machine</p>
<p>def compress(index):
return (index, index ** 2)</p>
<p>optimize(
fn=compress,
inputs=list(range(100)),
num_workers=2,
output_dir=”my_output”,
chunk_bytes=”64MB”,
num_nodes=2,
machine=Machine.DATA_PREP, # You can select between dozens of optimized machines
)</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> is a local path, the optimized dataset will be
present in: <code class="docutils literal notranslate"><span class="pre">/teamspace/jobs/{job_name}/nodes-0/my_output</span></code>. Otherwise,
it will be stored in the specified <code class="docutils literal notranslate"><span class="pre">output_dir</span></code>.</p>
<p>Read the optimized dataset:</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset</p>
<p>output_dir = “/teamspace/jobs/litdata-optimize-2024-07-08/nodes.0/my_output”</p>
<p>dataset = StreamingDataset(output_dir)</p>
<p>print(dataset[:])</p>
<p>.. raw:: html</p>
   </details>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Encrypt, decrypt data at chunk/sample level</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Secure data by applying encryption to individual samples or chunks,
ensuring sensitive information is protected during storage.</p>
<p>This example shows how to use the <code class="docutils literal notranslate"><span class="pre">FernetEncryption</span></code> class for
sample-level encryption with a data optimization function.</p>
<p>.. code:: python</p>
<p>from litdata import optimize
from litdata.utilities.encryption import FernetEncryption
import numpy as np
from PIL import Image</p>
</section>
<section id="initialize-fernetencryption-with-a-password-for-sample-level-encryption">
<h1>Initialize FernetEncryption with a password for sample-level encryption<a class="headerlink" href="#initialize-fernetencryption-with-a-password-for-sample-level-encryption" title="Permalink to this heading">¶</a></h1>
<p>fernet = FernetEncryption(password=”your_secure_password”, level=”sample”)
data_dir = “s3://my-bucket/optimized_data”</p>
<p>def random_image(index):
“””Generate a random image for demonstration purposes.”””
fake_img = Image.fromarray(np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8))
return {“image”: fake_img, “class”: index}</p>
</section>
<section id="optimize-data-while-applying-encryption">
<h1>Optimize data while applying encryption<a class="headerlink" href="#optimize-data-while-applying-encryption" title="Permalink to this heading">¶</a></h1>
<p>optimize(
fn=random_image,
inputs=list(range(5)),  # Example inputs: [0, 1, 2, 3, 4]
num_workers=1,
output_dir=data_dir,
chunk_bytes=”64MB”,
encryption=fernet,
)</p>
</section>
<section id="save-the-encryption-key-to-a-file-for-later-use">
<h1>Save the encryption key to a file for later use<a class="headerlink" href="#save-the-encryption-key-to-a-file-for-later-use" title="Permalink to this heading">¶</a></h1>
<p>fernet.save(“fernet.pem”)</p>
<p>Load the encrypted data using the <code class="docutils literal notranslate"><span class="pre">StreamingDataset</span></code> class as follows:</p>
<p>.. code:: python</p>
<p>from litdata import StreamingDataset
from litdata.utilities.encryption import FernetEncryption</p>
</section>
<section id="load-the-encryption-key">
<h1>Load the encryption key<a class="headerlink" href="#load-the-encryption-key" title="Permalink to this heading">¶</a></h1>
<p>fernet = FernetEncryption(password=”your_secure_password”, level=”sample”)
fernet.load(“fernet.pem”)</p>
</section>
<section id="create-a-streaming-dataset-for-reading-the-encrypted-samples">
<h1>Create a streaming dataset for reading the encrypted samples<a class="headerlink" href="#create-a-streaming-dataset-for-reading-the-encrypted-samples" title="Permalink to this heading">¶</a></h1>
<p>ds = StreamingDataset(input_dir=data_dir, encryption=fernet)</p>
<p>Implement your own encryption method: Subclass the <code class="docutils literal notranslate"><span class="pre">Encryption</span></code> class
and define the necessary methods:</p>
<p>.. code:: python</p>
<p>from litdata.utilities.encryption import Encryption</p>
<p>class CustomEncryption(Encryption):
def encrypt(self, data):
# Implement your custom encryption logic here
return data</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   def decrypt(self, data):
       # Implement your custom decryption logic here
       return data
</pre></div>
</div>
<p>This allows the data to remain secure while maintaining flexibility in
the encryption method.</p>
<p>.. raw:: html</p>
   </details>
<p></p>
<section id="features-for-transforming-datasets">
<h2>Features for transforming datasets<a class="headerlink" href="#features-for-transforming-datasets" title="Permalink to this heading">¶</a></h2>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>✅ Parallelize data transformations (map)</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<p>Apply the same change to different parts of the dataset at once to save
time and effort.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">map</span></code> operator can be used to apply a function over a list of
inputs.</p>
<p>Here is an example where the <code class="docutils literal notranslate"><span class="pre">map</span></code> operator is used to apply a
<code class="docutils literal notranslate"><span class="pre">resize_image</span></code> function over a folder of large images.</p>
<p>.. code:: python</p>
<p>from litdata import map
from PIL import Image</p>
</section>
</section>
<section id="note-inputs-could-also-refer-to-files-on-s3-directly">
<h1>Note: Inputs could also refer to files on s3 directly.<a class="headerlink" href="#note-inputs-could-also-refer-to-files-on-s3-directly" title="Permalink to this heading">¶</a></h1>
<p>input_dir = “my_large_images”
inputs = [os.path.join(input_dir, f) for f in os.listdir(input_dir)]</p>
<p># The resize image takes one of the input (image_path) and the output directory.</p>
</section>
<section id="files-written-to-output-dir-are-persisted">
<h1>Files written to output_dir are persisted.<a class="headerlink" href="#files-written-to-output-dir-are-persisted" title="Permalink to this heading">¶</a></h1>
<p>def resize_image(image_path, output_dir):
output_image_path = os.path.join(output_dir, os.path.basename(image_path))
Image.open(image_path).resize((224, 224)).save(output_image_path)</p>
<p>map(
fn=resize_image,
inputs=inputs,
output_dir=”s3://my-bucket/my_resized_images”,
)</p>
<p>.. raw:: html</p>
   </details>
<p></p>
</section>
<hr class="docutils" />
<section id="benchmarks">
<h1>Benchmarks<a class="headerlink" href="#benchmarks" title="Permalink to this heading">¶</a></h1>
<p>In this section we show benchmarks for speed to optimize a dataset and
the resulting streaming speed (<code class="docutils literal notranslate"><span class="pre">Reproduce</span> <span class="pre">the</span> <span class="pre">benchmark</span> <span class="pre">&lt;https://lightning.ai/lightning-ai/studios/benchmark-cloud-data-loading-libraries&gt;</span></code>__).</p>
<section id="streaming-speed">
<h2>Streaming speed<a class="headerlink" href="#streaming-speed" title="Permalink to this heading">¶</a></h2>
<p>Data optimized and streamed with LitData achieves a 20x speed up over
non optimized data and 2x speed up over other streaming solutions.</p>
<p>Speed to stream Imagenet 1.2M from AWS S3:</p>
<p>+————-+————-+————-+————-+————-+
| Framework   | Images /    | Images /    | Images /    | Images /    |
|             | sec 1st     | sec 2nd     | sec 1st     | sec 2nd     |
|             | Epoch       | Epoch       | Epoch       | Epoch       |
|             | (float32)   | (float32)   | (torch16)   | (torch16)   |
+=============+=============+=============+=============+=============+
| LitData     | <strong>5800</strong>    | <strong>6589</strong>    | <strong>6282</strong>    | <strong>7221</strong>    |
+————-+————-+————-+————-+————-+
| Web Dataset | 3134        | 3924        | 3343        | 4424        |
+————-+————-+————-+————-+————-+
| Mosaic ML   | 2898        | 5099        | 2809        | 5158        |
+————-+————-+————-+————-+————-+</p>
<p>.. raw:: html</p>
   <details>
<p>.. raw:: html</p>
   <summary>
<p>Benchmark details</p>
<p>.. raw:: html</p>
   </summary>
<p></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Imagenet-1.2M</span> <span class="pre">dataset</span> <span class="pre">&lt;https://www.image-net.org/&gt;</span></code>__ contains
<code class="docutils literal notranslate"><span class="pre">1,281,167</span> <span class="pre">images</span></code>.</p></li>
<li><p>To align with other benchmarks, we measured the streaming speed
(<code class="docutils literal notranslate"><span class="pre">images</span> <span class="pre">per</span> <span class="pre">second</span></code>) loaded from <code class="docutils literal notranslate"><span class="pre">AWS</span> <span class="pre">S3</span> <span class="pre">&lt;https://aws.amazon.com/s3/&gt;</span></code>__ for several frameworks.</p></li>
</ul>
<p>.. raw:: html</p>
   </details>
<p></p>
</section>
<section id="time-to-optimize-data">
<h2>Time to optimize data<a class="headerlink" href="#time-to-optimize-data" title="Permalink to this heading">¶</a></h2>
<p>LitData optimizes the Imagenet dataset for fast training 3-5x faster
than other frameworks:</p>
<p>Time to optimize 1.2 million ImageNet images (Faster is better): |
Framework |Train Conversion Time | Val Conversion Time | Dataset Size
| # Files | |—|—|—|—|—| | LitData | <strong>10:05 min</strong> | <strong>00:30
min</strong> | <strong>143.1 GB</strong> | 2.339 | | Web Dataset | 32:36 min | 01:22
min | 147.8 GB | 1.144 | | Mosaic ML | 49:49 min | 01:04 min |
<strong>143.1 GB</strong> | 2.298 |</p>
<p></p>
</section>
</section>
<hr class="docutils" />
<section id="parallelize-transforms-and-data-optimization-on-cloud-machines">
<h1>Parallelize transforms and data optimization on cloud machines<a class="headerlink" href="#parallelize-transforms-and-data-optimization-on-cloud-machines" title="Permalink to this heading">¶</a></h1>
<p>.. container::</p>
<section id="parallelize-data-transforms">
<h2>Parallelize data transforms<a class="headerlink" href="#parallelize-data-transforms" title="Permalink to this heading">¶</a></h2>
<p>Transformations with LitData are linearly parallelizable across
machines.</p>
<p>For example, let’s say that it takes 56 hours to embed a dataset on a
single A10G machine. With LitData, this can be speed up by adding more
machines in parallel</p>
<p>================== =====
Number of machines Hours
================== =====
1                  56
2                  28
4                  14
…                  …
64                 0.875
================== =====</p>
<p>To scale the number of machines, run the processing script on <code class="docutils literal notranslate"><span class="pre">Lightning</span> <span class="pre">Studios</span> <span class="pre">&lt;https://lightning.ai/&gt;</span></code>__:</p>
<p>.. code:: python</p>
<p>from litdata import map, Machine</p>
<p>map(
…
num_nodes=32,
machine=Machine.DATA_PREP, # Select between dozens of optimized machines
)</p>
</section>
<section id="parallelize-data-optimization">
<h2>Parallelize data optimization<a class="headerlink" href="#parallelize-data-optimization" title="Permalink to this heading">¶</a></h2>
<p>To scale the number of machines for data optimization, use <code class="docutils literal notranslate"><span class="pre">Lightning</span> <span class="pre">Studios</span> <span class="pre">&lt;https://lightning.ai/&gt;</span></code>__:</p>
<p>.. code:: python</p>
<p>from litdata import optimize, Machine</p>
<p>optimize(
…
num_nodes=32,
machine=Machine.DATA_PREP, # Select between dozens of optimized machines
)</p>
<p></p>
<p>Example: <code class="docutils literal notranslate"><span class="pre">Process</span> <span class="pre">the</span> <span class="pre">LAION</span> <span class="pre">400</span> <span class="pre">million</span> <span class="pre">image</span> <span class="pre">dataset</span> <span class="pre">in</span> <span class="pre">2</span> <span class="pre">hours</span> <span class="pre">on</span> <span class="pre">32</span> <span class="pre">machines,</span> <span class="pre">each</span> <span class="pre">with</span> <span class="pre">32</span> <span class="pre">CPUs</span> <span class="pre">&lt;https://lightning.ai/lightning-ai/studios/use-or-explore-laion-400million-dataset&gt;</span></code>__.</p>
<p></p>
</section>
</section>
<hr class="docutils" />
<section id="start-from-a-template">
<h1>Start from a template<a class="headerlink" href="#start-from-a-template" title="Permalink to this heading">¶</a></h1>
<p>Below are templates for real-world applications of LitData at scale.</p>
<section id="templates-transform-datasets">
<h2>Templates: Transform datasets<a class="headerlink" href="#templates-transform-datasets" title="Permalink to this heading">¶</a></h2>
<p>+————————-+———–+———–+———+———+
| Studio                  | Data type | Time      | M       | Dataset |
|                         |           | (minutes) | achines |         |
+=========================+===========+===========+=========+=========+
| <code class="docutils literal notranslate"><span class="pre">Download</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">Image</span> <span class="pre">&amp;</span>&#160;&#160; <span class="pre">|</span> <span class="pre">120</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">32</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> </code>LAION  |
| LAION-400MILLION        | Text      |           |         | -400M &lt; |
| da                      |           |           |         | https:/ |
| taset &lt;https://lightnin |           |           |         | /laion. |
| g.ai/lightning-ai/studi |           |           |         | ai/blog |
| os/use-or-explore-laion |           |           |         | /laion- |
| -400million-dataset&gt;<code class="docutils literal notranslate"><span class="pre">__</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">400-ope</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">n-datas</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">et/&gt;</span></code>__ |
+————————-+———–+———–+———+———+
| <code class="docutils literal notranslate"><span class="pre">Tokenize</span> <span class="pre">2M</span> <span class="pre">Swedish</span>&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">Text</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">7</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">4</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> </code>       |
| Wikipedia               |           |           |         | Swedish |
| Ar                      |           |           |         | Wikiped |
| ticles &lt;https://lightni |           |           |         | ia &lt;htt |
| ng.ai/lightning-ai/stud |           |           |         | ps://hu |
| ios/tokenize-2m-swedish |           |           |         | ggingfa |
| -wikipedia-articles&gt;<code class="docutils literal notranslate"><span class="pre">__</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ce.co/d</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">atasets</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">/wikipe</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">dia&gt;</span></code>__ |
+————————-+———–+———–+———+———+
| <code class="docutils literal notranslate"><span class="pre">Embed</span> <span class="pre">English</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">Text</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">15</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">3</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> </code>       |
| Wikipedia under 5       |           |           |         | English |
| do                      |           |           |         | Wikiped |
| llars &lt;https://lightnin |           |           |         | ia &lt;htt |
| g.ai/lightning-ai/studi |           |           |         | ps://hu |
| os/embed-english-wikipe |           |           |         | ggingfa |
| dia-under-5-dollars&gt;<code class="docutils literal notranslate"><span class="pre">__</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ce.co/d</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">atasets</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">/wikipe</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">dia&gt;</span></code>__ |
+————————-+———–+———–+———+———+</p>
</section>
<section id="templates-optimize-stream-data">
<h2>Templates: Optimize + stream data<a class="headerlink" href="#templates-optimize-stream-data" title="Permalink to this heading">¶</a></h2>
<p>+———————–+————+————+———+———+
| Studio                | Data type  | Time       | M       | Dataset |
|                       |            | (minutes)  | achines |         |
+=======================+============+============+=========+=========+
| <code class="docutils literal notranslate"><span class="pre">Benchmark</span> <span class="pre">cloud</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">Image</span> <span class="pre">&amp;</span>&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">10</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">1</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> </code>I      |
| data-loading          | Label      |            |         | magenet |
| libraries &lt;           |            |            |         | 1M      |
| https://lightning.ai/ |            |            |         | &lt;https: |
| lightning-ai/studios/ |            |            |         | //paper |
| benchmark-cloud-data- |            |            |         | swithco |
| loading-libraries&gt;<code class="docutils literal notranslate"><span class="pre">__</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">de.com/</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">sota/im</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">age-cla</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ssifica</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">tion-on</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">-imagen</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">et?tag_</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">filter=</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">171&gt;</span></code>__ |
+———————–+————+————+———+———+
| <code class="docutils literal notranslate"><span class="pre">Optimize</span> <span class="pre">GeoSpatial</span>&#160; <span class="pre">|</span> <span class="pre">Image</span> <span class="pre">&amp;</span>&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">120</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">32</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> </code>Che    |
| data for model        | Mask       |            |         | sapeake |
| training &lt;https       |            |            |         | Roads   |
| ://lightning.ai/light |            |            |         | Spatial |
| ning-ai/studios/conve |            |            |         | C       |
| rt-spatial-data-to-li |            |            |         | ontext  |
| ghtning-streaming&gt;<code class="docutils literal notranslate"><span class="pre">__</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">&lt;https:</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">//githu</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">b.com/i</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">saaccor</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ley/che</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">sapeake</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">rsc&gt;</span></code>__ |
+———————–+————+————+———+———+
| <code class="docutils literal notranslate"><span class="pre">Optimize</span> <span class="pre">TinyLlama</span>&#160;&#160; <span class="pre">|</span> <span class="pre">Text</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">240</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">32</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> </code>Sl     |
| 1T dataset for        |            |            |         | imPajam |
| training &lt;            |            |            |         | a &lt;http |
| https://lightning.ai/ |            |            |         | s://hug |
| lightning-ai/studios/ |            |            |         | gingfac |
| prepare-the-tinyllama |            |            |         | e.co/da |
| -1t-token-dataset&gt;<code class="docutils literal notranslate"><span class="pre">__</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">tasets/</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">cerebra</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">s/SlimP</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ajama-6</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">27B&gt;</span></code>__ |
|                       |            |            |         | &amp;       |
|                       |            |            |         | <code class="docutils literal notranslate"><span class="pre">StarC</span>&#160; <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">oder</span> <span class="pre">&lt;h</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ttps://</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">hugging</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">face.co</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">/datase</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ts/bigc</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ode/sta</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">rcoderd</span> <span class="pre">|</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">ata&gt;</span></code>__ |
+———————–+————+————+———+———+
| <code class="docutils literal notranslate"><span class="pre">Optimize</span> <span class="pre">parquet</span>&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">Parquet</span>&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">12</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">16</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">R</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">|</span> <span class="pre">files</span> <span class="pre">for</span> <span class="pre">model</span>&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">Files</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">andomly</span> <span class="pre">|</span> <span class="pre">|</span> <span class="pre">training</span> <span class="pre">&lt;h</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">Ge</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">|</span> <span class="pre">ttps://lightning.ai/l</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">nerated</span> <span class="pre">|</span> <span class="pre">|</span> <span class="pre">ightning-ai/studios/c</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">data</span>&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">|</span> <span class="pre">onvert-parquets-to-li</span> <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">|</span> <span class="pre">|</span> <span class="pre">ghtning-streaming&gt;</span></code>__ |            |            |         |         |
+———————–+————+————+———+———+</p>
<p></p>
</section>
</section>
<hr class="docutils" />
<section id="community">
<h1>Community<a class="headerlink" href="#community" title="Permalink to this heading">¶</a></h1>
<p>LitData is a community project accepting contributions - Let’s make the
world’s most advanced AI data processing framework.</p>
<p>| 💬 <code class="docutils literal notranslate"><span class="pre">Get</span> <span class="pre">help</span> <span class="pre">on</span> <span class="pre">Discord</span> <span class="pre">&lt;https://discord.com/invite/XncpTy7DSt&gt;</span></code>__
| 📋 <code class="docutils literal notranslate"><span class="pre">License:</span> <span class="pre">Apache</span>&#160;&#160; <span class="pre">2.0</span> <span class="pre">&lt;https://github.com/Lightning-AI/litdata/blob/main/LICENSE&gt;</span></code>__</p>
<p>.. |PyPI| image:: https://img.shields.io/pypi/v/litdata
.. |Downloads| image:: https://img.shields.io/pypi/dm/litdata
.. |License| image:: https://img.shields.io/github/license/Lightning-AI/litdata</p>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="index.html" class="btn btn-neutral" title="lit-data" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2023-2024, Lightning AI et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Transform data at scale. Optimize for fast model training.</a></li>
<li><a class="reference internal" href="#quick-start">Quick start</a></li>
<li><a class="reference internal" href="#speed-up-model-training">Speed up model training</a></li>
<li><a class="reference internal" href="#transform-datasets">Transform datasets</a></li>
<li><a class="reference internal" href="#use-a-local-or-s3-folder">use a local or S3 folder</a></li>
<li><a class="reference internal" href="#resize-the-input-image">resize the input image</a></li>
<li><a class="reference internal" href="#key-features">Key Features</a><ul>
<li><a class="reference internal" href="#features-for-optimizing-and-streaming-datasets-for-model-training">Features for optimizing and streaming datasets for model training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#initialize-the-streamingdataset-with-the-custom-cache-directory">Initialize the StreamingDataset with the custom cache directory</a></li>
<li><a class="reference internal" href="#for-the-training-dataset-don-t-forget-to-enable-shuffle-and-drop-last">For the training dataset, don’t forget to enable shuffle and drop_last !!!</a></li>
<li><a class="reference internal" href="#read-data-from-aws-s3">Read data from AWS S3</a></li>
<li><a class="reference internal" href="#read-data-from-gcs">Read data from GCS</a></li>
<li><a class="reference internal" href="#read-data-from-azure">Read data from Azure</a></li>
<li><a class="reference internal" href="#iterate-over-the-data">Iterate over the data</a></li>
<li><a class="reference internal" href="#define-a-function-to-convert-the-text-within-the-jsonl-files-into-tokens">1. Define a function to convert the text within the jsonl files into tokens</a></li>
<li><a class="reference internal" href="#increase-by-one-because-we-need-the-next-word-as-well">Increase by one because we need the next word as well</a></li>
<li><a class="reference internal" href="#iterate-over-the-slimpajama-dataset">Iterate over the SlimPajama dataset</a></li>
<li><a class="reference internal" href="#mix-slimpajama-data-and-starcoder-data-with-these-proportions">Mix SlimPajama data and Starcoder data with these proportions:</a></li>
<li><a class="reference internal" href="#iterate-over-the-combined-datasets">Iterate over the combined datasets</a></li>
<li><a class="reference internal" href="#define-a-function-to-convert-the-text-within-the-parquet-files-into-tokens">1. Define a function to convert the text within the parquet files into tokens</a></li>
<li><a class="reference internal" href="#generate-the-inputs">2. Generate the inputs</a></li>
<li><a class="reference internal" href="#store-the-optimized-data-wherever-you-want-under-teamspace-datasets-or-teamspace-s3-connections">3. Store the optimized data wherever you want under “/teamspace/datasets” or “/teamspace/s3_connections”</a></li>
<li><a class="reference internal" href="#initialize-fernetencryption-with-a-password-for-sample-level-encryption">Initialize FernetEncryption with a password for sample-level encryption</a></li>
<li><a class="reference internal" href="#optimize-data-while-applying-encryption">Optimize data while applying encryption</a></li>
<li><a class="reference internal" href="#save-the-encryption-key-to-a-file-for-later-use">Save the encryption key to a file for later use</a></li>
<li><a class="reference internal" href="#load-the-encryption-key">Load the encryption key</a></li>
<li><a class="reference internal" href="#create-a-streaming-dataset-for-reading-the-encrypted-samples">Create a streaming dataset for reading the encrypted samples</a><ul>
<li><a class="reference internal" href="#features-for-transforming-datasets">Features for transforming datasets</a></li>
</ul>
</li>
<li><a class="reference internal" href="#note-inputs-could-also-refer-to-files-on-s3-directly">Note: Inputs could also refer to files on s3 directly.</a></li>
<li><a class="reference internal" href="#files-written-to-output-dir-are-persisted">Files written to output_dir are persisted.</a></li>
<li><a class="reference internal" href="#benchmarks">Benchmarks</a><ul>
<li><a class="reference internal" href="#streaming-speed">Streaming speed</a></li>
<li><a class="reference internal" href="#time-to-optimize-data">Time to optimize data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#parallelize-transforms-and-data-optimization-on-cloud-machines">Parallelize transforms and data optimization on cloud machines</a><ul>
<li><a class="reference internal" href="#parallelize-data-transforms">Parallelize data transforms</a></li>
<li><a class="reference internal" href="#parallelize-data-optimization">Parallelize data optimization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#start-from-a-template">Start from a template</a><ul>
<li><a class="reference internal" href="#templates-transform-datasets">Templates: Transform datasets</a></li>
<li><a class="reference internal" href="#templates-optimize-stream-data">Templates: Optimize + stream data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#community">Community</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources"> -->
    <!-- <div class="container"> -->
      <!-- <div class="row"> -->
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning-ai.github.io/lit-data/">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://lightning.ai">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://lightning.ai">View Resources</a>
        </div>
        -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://lightning-ai.github.io/lit-data/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning-ai.github.io/lit-data/">PyTorch</a></li>
            <li><a href="https://lightning.ai">Get Started</a></li>
            <li><a href="https://lightning-ai.github.io/lit-data/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.Lightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning.ai">Resources</a></li>
            <li><a href="https://lightning.ai">Tutorials</a></li>
            <li><a href="https://lightning-ai.github.io/lit-data/">Docs</a></li>
            <li><a href="https://discord.com/invite/tfXFetEZxv" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/Lightning-AI/lit-data/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/LightningAI" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://lightning-ai.github.io/lit-data/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://lightning.ai">Get Started</a>
          </li>

          <li>
            <a href="https://www.Lightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Lightning Fabric</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Fabric</a>
            </li>
          </ul>

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai">Developer Resources</a>
            </li>

            <li>
              <a href="https://lightning-ai.github.io/lit-data/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="">Community</a>
            </li>

            <li>
              <a href="">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://github.com/Lightning-AI/lit-data">Github</a>
          </li>

          <li>
            <a href="https://www.lightning.ai/">Lightning.ai</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PQBQ3CV"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>